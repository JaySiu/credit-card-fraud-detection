{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Reading input file and storing to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up AWS S3 access credentials\n",
    "ACCESS_KEY = \"AKIAI5AUGUGKHT2VCQ5Q\"\n",
    "SECRET_KEY = \"aWlGdedlHIR5asFQE2vAF5l3LPdfoYL5SMMgejkk\"\n",
    "ENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\n",
    "AWS_BUCKET_NAME = \"creditfrauddata\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read from s3\n",
    "2. split columns\n",
    "3. filter header\n",
    "4. filter empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fileName = 'PS_20174392719_1491204439457_log.csv'\n",
    "\n",
    "def convertToTransactionSchema(arr):\n",
    "    res = arr\n",
    "    res[0] = int(arr[0]) # step\n",
    "    res[2] = float(arr[2]) # amount\n",
    "    res[4] = float(arr[4]) # old balance\n",
    "    res[5] = float(arr[5]) # new balance\n",
    "    res[7] = float(arr[7]) # old balance destination\n",
    "    res[8] = float(arr[8]) # new balance destination\n",
    "    res[9] = int(arr[9]) # is fraud\n",
    "    res[10] = int(arr[10]) # is flagged fraud\n",
    "    return res\n",
    "\n",
    "transactionsRDD = sc.textFile(\"s3n://%s:%s@%s/%s\" % (ACCESS_KEY, SECRET_KEY, AWS_BUCKET_NAME, fileName)) \\\n",
    "    .map(lambda line: line.split(\",\")) \\\n",
    "    .filter(lambda line: line[0] != \"step\") \\\n",
    "    .filter(lambda line: len(line)>1) \\\n",
    "    .map(convertToTransactionSchema)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(step=1, type=u'PAYMENT', amount=9839.6396484375, nameOrig=u'C1231006815', oldbalanceOrg=170136.0, newbalanceOrig=160296.359375, nameDest=u'M1979787155', oldbalanceDest=0.0, newbalanceDest=0.0, isFraud=0, isFlaggedFraud=0), Row(step=1, type=u'PAYMENT', amount=1864.280029296875, nameOrig=u'C1666544295', oldbalanceOrg=21249.0, newbalanceOrig=19384.720703125, nameDest=u'M2044282225', oldbalanceDest=0.0, newbalanceDest=0.0, isFraud=0, isFlaggedFraud=0), Row(step=1, type=u'TRANSFER', amount=181.0, nameOrig=u'C1305486145', oldbalanceOrg=181.0, newbalanceOrig=0.0, nameDest=u'C553264065', oldbalanceDest=0.0, newbalanceDest=0.0, isFraud=1, isFlaggedFraud=0)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "transactionSchema = StructType([\n",
    "    StructField(\"step\", IntegerType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"amount\", FloatType(), True),\n",
    "    StructField(\"nameOrig\", StringType(), True),\n",
    "    StructField(\"oldbalanceOrg\", FloatType(), True),\n",
    "    StructField(\"newbalanceOrig\", FloatType(), True),\n",
    "    StructField(\"nameDest\", StringType(), True),\n",
    "    StructField(\"oldbalanceDest\", FloatType(), True),\n",
    "    StructField(\"newbalanceDest\", FloatType(), True),\n",
    "    StructField(\"isFraud\", IntegerType(), True),\n",
    "    StructField(\"isFlaggedFraud\", IntegerType(), True)])\n",
    "\n",
    "transactionsDF = sqlContext.createDataFrame(transactionsRDD, transactionSchema).cache()\n",
    "print transactionsDF.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- amount: float (nullable = true)\n",
      " |-- nameOrig: string (nullable = true)\n",
      " |-- oldbalanceOrg: float (nullable = true)\n",
      " |-- newbalanceOrig: float (nullable = true)\n",
      " |-- nameDest: string (nullable = true)\n",
      " |-- oldbalanceDest: float (nullable = true)\n",
      " |-- newbalanceDest: float (nullable = true)\n",
      " |-- isFraud: integer (nullable = true)\n",
      " |-- isFlaggedFraud: integer (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print transactionsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check each column for invalid values. \n",
    "\n",
    "For columns with string type: empty or null values.\n",
    "For integer or float: null or minus values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "\n",
    "stringCols = ['type', 'nameOrig', 'nameDest']\n",
    "nonStringCols = [item for item in transactionsDF.columns if item not in stringCols]\n",
    "\n",
    "transactionsDF.select([count(when(col(c).isNull() | (col(c) == ''), c)).alias(c) for c in stringCols]).show()\n",
    "transactionsDF.select([count(when(col(c).isNull() | (col(c) < 0), c)).alias(c) for c in nonStringCols]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Building the fraud detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Evaluating the fraud detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Analyzing the fraud detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
