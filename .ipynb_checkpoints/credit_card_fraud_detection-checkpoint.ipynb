{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FINANCIAL FRAUD DETECTION**\n",
    "===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Reading the input data file and storing to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import sys\n",
    "import os\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Read input file and convert it to RDD.\n",
    "1. Read from file system\n",
    "2. split columns\n",
    "3. filter header\n",
    "4. filter empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = '/vagrant/PS_20174392719_1491204439457_log.csv'\n",
    "\n",
    "def convertToTransactionSchema(arr):\n",
    "    res = arr\n",
    "    res[0] = int(arr[0]) # step\n",
    "    res[2] = float(arr[2]) # amount\n",
    "    res[4] = float(arr[4]) # old balance\n",
    "    res[5] = float(arr[5]) # new balance\n",
    "    res[7] = float(arr[7]) # old balance destination\n",
    "    res[8] = float(arr[8]) # new balance destination\n",
    "    res[9] = int(arr[9]) # is fraud\n",
    "    res[10] = int(arr[10]) # is flagged fraud\n",
    "    return res\n",
    "\n",
    "transactionsRDD = (sc.textFile(filename)\n",
    "       .map(lambda line: line.split(\",\"))\n",
    "       .filter(lambda line: line[0] != \"step\")\n",
    "       .filter(lambda line: len(line)>1)\n",
    "       .map(convertToTransactionSchema))\n",
    "transactionsRDD.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "###Convert transactionsRDD into DataFrame and cache it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(step=1, type=u'PAYMENT', amount=9839.6396484375, nameOrig=u'C1231006815', oldbalanceOrig=170136.0, newbalanceOrig=160296.359375, nameDest=u'M1979787155', oldbalanceDest=0.0, newbalanceDest=0.0, isFraud=0, isFlaggedFraud=0), Row(step=1, type=u'PAYMENT', amount=1864.280029296875, nameOrig=u'C1666544295', oldbalanceOrig=21249.0, newbalanceOrig=19384.720703125, nameDest=u'M2044282225', oldbalanceDest=0.0, newbalanceDest=0.0, isFraud=0, isFlaggedFraud=0)]\n"
     ]
    }
   ],
   "source": [
    "transactionSchema = StructType([\n",
    "    StructField(\"step\", IntegerType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"amount\", FloatType(), True),\n",
    "    StructField(\"nameOrig\", StringType(), True),\n",
    "    StructField(\"oldbalanceOrig\", FloatType(), True),\n",
    "    StructField(\"newbalanceOrig\", FloatType(), True),\n",
    "    StructField(\"nameDest\", StringType(), True),\n",
    "    StructField(\"oldbalanceDest\", FloatType(), True),\n",
    "    StructField(\"newbalanceDest\", FloatType(), True),\n",
    "    StructField(\"isFraud\", IntegerType(), True),\n",
    "    StructField(\"isFlaggedFraud\", IntegerType(), True)])\n",
    "\n",
    "transactionsDF = sqlContext.createDataFrame(transactionsRDD, transactionSchema)\n",
    "transactionsDF.cache()\n",
    "print transactionsDF.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Check if the DataFrame is cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactionsDF.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###The purpose of data cleaning is to check if there is any invalid or missing values, and eliminate those reords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Print the datatype for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- amount: float (nullable = true)\n",
      " |-- nameOrig: string (nullable = true)\n",
      " |-- oldbalanceOrig: float (nullable = true)\n",
      " |-- newbalanceOrig: float (nullable = true)\n",
      " |-- nameDest: string (nullable = true)\n",
      " |-- oldbalanceDest: float (nullable = true)\n",
      " |-- newbalanceDest: float (nullable = true)\n",
      " |-- isFraud: integer (nullable = true)\n",
      " |-- isFlaggedFraud: integer (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print transactionsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###We need to check each column for invalid values. The set of invalid values depends on the data type of the columns:\n",
    "1. For columns with string type: empty or null values.\n",
    "2. Otherwise: null or minus values.\n",
    "\n",
    "###Firstly, we need to get columns that has 'string' datatype and store it in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['type', 'nameOrig', 'nameDest']\n"
     ]
    }
   ],
   "source": [
    "stringCols = []\n",
    "for (colName, colType) in transactionsDF.dtypes:\n",
    "    if(colType == 'string'):\n",
    "        stringCols.append(colName)\n",
    "        \n",
    "print stringCols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Then, we count the number of invalid values in each column and show the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step\n",
      "type\n",
      "amount\n",
      "nameOrig\n",
      "oldbalanceOrig\n",
      "newbalanceOrig\n",
      "nameDest\n",
      "oldbalanceDest\n",
      "newbalanceDest\n",
      "isFraud\n",
      "isFlaggedFraud\n",
      "+----+----+------+--------+--------------+--------------+--------+--------------+--------------+-------+--------------+\n",
      "|step|type|amount|nameOrig|oldbalanceOrig|newbalanceOrig|nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
      "+----+----+------+--------+--------------+--------------+--------+--------------+--------------+-------+--------------+\n",
      "|   0|   0|     0|       0|             0|             0|       0|             0|             0|      0|             0|\n",
      "+----+----+------+--------+--------------+--------------+--------+--------------+--------------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "#from IPython.display import display\n",
    "\n",
    "def isInvalidValue(colName):\n",
    "    print(colName)\n",
    "    if colName in stringCols:\n",
    "        return (col(colName).isNull() | (col(colName) == ''))\n",
    "    else:\n",
    "        return (col(colName).isNull() | (col(colName) < 0))\n",
    "\n",
    "transactionsDF.select([count(when(isInvalidValue(c), c)).alias(c) for c in transactionsDF.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###As shown, the dataset does not contain any invalid values so we do not need to elminate any records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###First, we would explore some basic information in the dataset:\n",
    "\n",
    "1. Number of fraudulent transactions\n",
    "2. Types of the fraudulent transactions\n",
    "3. What determines whether the feature isFlaggedFraud gets set or not?\n",
    "4. Statistics of each column\n",
    "5. Correllation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1. Number and percentage of fraudulent transactions and show the records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---------+-----------+--------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|step|    type|   amount|   nameOrig|oldbalanceOrig|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
      "+----+--------+---------+-----------+--------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|   1|TRANSFER|    181.0|C1305486145|         181.0|           0.0| C553264065|           0.0|           0.0|      1|             0|\n",
      "|   1|CASH_OUT|    181.0| C840083671|         181.0|           0.0|  C38997010|       21182.0|           0.0|      1|             0|\n",
      "|   1|TRANSFER|   2806.0|C1420196421|        2806.0|           0.0| C972765878|           0.0|           0.0|      1|             0|\n",
      "|   1|CASH_OUT|   2806.0|C2101527076|        2806.0|           0.0|C1007251739|       26202.0|           0.0|      1|             0|\n",
      "|   1|TRANSFER|  20128.0| C137533655|       20128.0|           0.0|C1848415041|           0.0|           0.0|      1|             0|\n",
      "|   1|CASH_OUT|  20128.0|C1118430673|       20128.0|           0.0| C339924917|        6268.0|      12145.85|      1|             0|\n",
      "|   1|CASH_OUT|416001.34| C749981943|           0.0|           0.0| C667346055|         102.0|     9291620.0|      1|             0|\n",
      "|   1|TRANSFER|1277212.8|C1334405552|     1277212.8|           0.0| C431687661|           0.0|           0.0|      1|             0|\n",
      "|   1|CASH_OUT|1277212.8| C467632528|     1277212.8|           0.0| C716083600|           0.0|     2444985.2|      1|             0|\n",
      "|   1|TRANSFER| 35063.63|C1364127192|      35063.63|           0.0|C1136419747|           0.0|           0.0|      1|             0|\n",
      "|   1|CASH_OUT| 35063.63|C1635772897|      35063.63|           0.0|C1983025922|       31140.0|       7550.03|      1|             0|\n",
      "|   1|TRANSFER| 25071.46| C669700766|      25071.46|           0.0|C1384210339|           0.0|           0.0|      1|             0|\n",
      "|   1|CASH_OUT| 25071.46|C1275464847|      25071.46|           0.0|C1364913072|       9083.76|      34155.22|      1|             0|\n",
      "|   1|CASH_OUT|132842.64|  C13692003|       4499.08|           0.0| C297927961|           0.0|     132842.64|      1|             0|\n",
      "|   1|TRANSFER|235238.66|C1872047468|     235238.66|           0.0| C116289363|           0.0|           0.0|      1|             0|\n",
      "|   1|CASH_OUT|235238.66|C1499825229|     235238.66|           0.0|C2100440237|           0.0|     235238.66|      1|             0|\n",
      "|   2|TRANSFER|1096187.2|C1093223281|     1096187.2|           0.0|C2063275841|           0.0|           0.0|      1|             0|\n",
      "|   2|CASH_OUT|1096187.2|  C77163673|     1096187.2|           0.0| C644345897|           0.0|     1096187.2|      1|             0|\n",
      "|   2|TRANSFER| 963532.1|C1440057381|      963532.1|           0.0| C268086000|           0.0|           0.0|      1|             0|\n",
      "|   2|CASH_OUT| 963532.1| C430329518|      963532.1|           0.0| C991505714|     132382.56|     1095914.8|      1|             0|\n",
      "+----+--------+---------+-----------+--------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "The total number of fraudulent records =  8213\n",
      "The percentage of fraudulent records = 0.129082%\n"
     ]
    }
   ],
   "source": [
    "fraudulentTransactions = transactionsDF.filter(transactionsDF.isFraud == 1)\n",
    "fraudulentTransactions.show()\n",
    "print \"The total number of fraudulent records = \", fraudulentTransactions.count()\n",
    "print \"The percentage of fraudulent records = %f%%\" % ( float(fraudulentTransactions.count())/transactionsDF.count()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2. Types of the fraudulent transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    type|count|\n",
      "+--------+-----+\n",
      "|CASH_OUT| 4116|\n",
      "|TRANSFER| 4097|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fraudulentTransactions.groupBy(fraudulentTransactions.type).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###We can see there are two fradulent types of transactions, CASH_OUT and TRANSFER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3. What determines whether the feature isFlaggedFraud gets set or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: double (nullable = true)\n",
      " |-- type: double (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- nameOrig: double (nullable = true)\n",
      " |-- oldbalanceOrig: double (nullable = true)\n",
      " |-- newbalanceOrig: double (nullable = true)\n",
      " |-- nameDest: double (nullable = true)\n",
      " |-- oldbalanceDest: double (nullable = true)\n",
      " |-- newbalanceDest: double (nullable = true)\n",
      " |-- isFraud: double (nullable = true)\n",
      " |-- isFlaggedFraud: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactionsDF.drop('isFlaggedFraud')\n",
    "categoricalTransactionsDF = transactionsDF.select([col(c).cast(\"double\").alias(c) for c in transactionsDF.columns])\n",
    "categoricalTransactionsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Building up the fraud detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o787.transform.\n: org.apache.spark.SparkException: VectorAssembler does not support the StringType type\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$2.apply(VectorAssembler.scala:88)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$2.apply(VectorAssembler.scala:54)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:108)\n\tat org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:54)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-c42899166352>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mfeaturesCols\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'isFraud'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mfeatureAssembler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputCols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeaturesCols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rawFeatures\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransactionsDF\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mfeatureIndexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVectorIndexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rawFeatures\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"features\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxCategories\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransactionsDF\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestData\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandomSplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    112\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Params must be either a param map but got %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    307\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o787.transform.\n: org.apache.spark.SparkException: VectorAssembler does not support the StringType type\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$2.apply(VectorAssembler.scala:88)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$2.apply(VectorAssembler.scala:54)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:108)\n\tat org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:54)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"isFraud\", outputCol=\"isFraudLabel\").fit(categoricalTransactionsDF)\n",
    "\n",
    "featuresCols = categoricalTransactionsDF.columns\n",
    "featuresCols.remove('isFraud')\n",
    "\n",
    "featureAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"rawFeatures\").transform(categoricalTransactionsDF)\n",
    "featureIndexer = VectorIndexer(inputCol=\"rawFeatures\", outputCol=\"features\", maxCategories=5).fit(categoricalTransactionsDF)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "randomForest = RandomForestClassifier(labelCol=\"isFraudLabel\", featuresCol=\"features\")\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureAssembler, featureIndexer, randomForest])\n",
    "model = pipeline.fit(trainingData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To make the string columns categorical, they should be casted into double datatype first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: double (nullable = true)\n",
      " |-- type: double (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- nameOrig: double (nullable = true)\n",
      " |-- oldbalanceOrig: double (nullable = true)\n",
      " |-- newbalanceOrig: double (nullable = true)\n",
      " |-- nameDest: double (nullable = true)\n",
      " |-- oldbalanceDest: double (nullable = true)\n",
      " |-- newbalanceDest: double (nullable = true)\n",
      " |-- isFraud: double (nullable = true)\n",
      " |-- isFlaggedFraud: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#categoricalTransactionsDF = transactionsDF.select([col(c).cast(\"double\").alias(c) for c in transactionsDF.columns if c in stringCols])\n",
    "transactionsDF.drop('isFlaggedFraud')\n",
    "categoricalTransactionsDF = transactionsDF.select([col(c).cast(\"double\").alias(c) for c in transactionsDF.columns])\n",
    "categoricalTransactionsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Spilting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4455162 training reords and 1907458 test records.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "trainingData, testData = transactionsRDD.randomSplit([0.7, 0.3])\n",
    "print \"There are %d training reords and %d test records.\" % (trainingData.count(), testData.count())\n",
    "print \"There are %d fraudulent records in the test data.\" % (testData.filter(transactionsDF.isFraud == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Evaluating the fraud detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Analyzing the fraud detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
